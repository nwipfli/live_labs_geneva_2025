{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMQCs0oQf5Jo"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR0jdheRGG89"
      },
      "source": [
        "# Text + multimodal embedding generation and vector search in BigQuery\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td9kx9LVgSve"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_embeddings_vector_search.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fapplying-llms-to-data%2Fbigquery_embeddings_vector_search.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/applying-llms-to-data/bigquery_embeddings_vector_search.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_embeddings_vector_search.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery Studio logo\"><br> Open in BigQuery Studio\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_embeddings_vector_search.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://raw.githubusercontent.com/primer/octicons/refs/heads/main/icons/mark-github-24.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_embeddings_vector_search.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_embeddings_vector_search.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_embeddings_vector_search.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_embeddings_vector_search.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/applying-llms-to-data/bigquery_embeddings_vector_search.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoWWwsHwSrCc"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Authors | [Alicia Williams](https://github.com/aliciawilliams), [Jeff Nelson](https://github.com/jeffonelson) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_md"
      },
      "source": [
        "## Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwQLISjqGTHd"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "In this tutorial, you'll build a [**semantic search**](https://cloud.google.com/discover/what-is-semantic-search) utility for a fictional e-commerce pet store. You will use [BigQuery](https://cloud.google.com/bigquery/docs) and [Vertex AI](https://cloud.google.com/vertex-ai/docs), to go beyond simple keyword matching by generating [**vector embeddings**](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings) for product text descriptions _and_ for images. This allows customers to find products based on conceptual meaning, not just keywords, creating a more intuitive shopping experience.\n",
        "\n",
        "By the end, you will know how to:\n",
        "\n",
        "* **Generate embeddings (numerical representations)** for text and image data.\n",
        "\n",
        "* **Perform text-to-text vector search** to find products with similar text descriptions\n",
        "\n",
        "* **Perform text-to-image vector search** to find product images that visually match a text query\n",
        "\n",
        "\n",
        "All within BigQuery!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "costs_md"
      },
      "source": [
        "### Services and Costs\n",
        "\n",
        "This tutorial uses the following billable components of Google Cloud:\n",
        "\n",
        "* **BigQuery**: [Pricing](https://cloud.google.com/bigquery/pricing)\n",
        "\n",
        "* **BigQuery ML**: [Pricing](https://cloud.google.com/bigquery/pricing#bqml)\n",
        "\n",
        "* **Vertex AI**: [Pricing](https://cloud.google.com/vertex-ai/generative-ai/pricing)\n",
        "\n",
        "You can use the [Pricing Calculator](https://cloud.google.com/products/calculator) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkoCFoFVSPii"
      },
      "source": [
        "---\n",
        "\n",
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ3g-h7uTaSf"
      },
      "source": [
        "### Set your project ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conn_md"
      },
      "source": [
        "### Create BigQuery Cloud resource connection\n",
        "\n",
        "To allow BigQuery to securely interact with other Google Cloud services like Vertex AI (where embedding models are hosted), you need to create a [Cloud resource connection](https://cloud.google.com/bigquery/docs/create-cloud-resource-connection). This code uses the `bq` command-line tool to create a `CLOUD_RESOURCE` connection named `cymbal_conn` in the `US` location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "conn_code"
      },
      "outputs": [],
      "source": [
        "!bq mk --connection --location=us \\\n",
        "    --connection_type=CLOUD_RESOURCE cymbal_conn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "perms_md"
      },
      "source": [
        "### Set permissions for Service Account\n",
        "\n",
        "The resource connection service account requires certain project-level permissions to interact with Vertex AI and Google Cloud Storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3iizFFY8503"
      },
      "source": [
        "First, you need to find the email address associated with your new connection's service account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OqpZNY953xR"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCT = !bq show --format=prettyjson --connection us.cymbal_conn | grep \"serviceAccountId\" | cut -d '\"' -f 4\n",
        "SERVICE_ACCT_EMAIL = SERVICE_ACCT[-1]\n",
        "print(SERVICE_ACCT_EMAIL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt9Tb-li9BSu"
      },
      "source": [
        "Next you'll assign roles that give their service account permission to be a \"BigQuery Connection User,\" an \"AI Platform User\" (so it can access Gemini models in Vertex AI), and a \"Storage Object Viewer\" (so it can access images in Cloud Storage).\n",
        "\n",
        "*A 60 second pause is added to allow the new IAM permissions to propagate throughout Google Cloud.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "perms_code"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "!gcloud projects add-iam-policy-binding --format=none $PROJECT_ID --member=serviceAccount:$SERVICE_ACCT_EMAIL --role='roles/bigquery.connectionUser'\n",
        "!gcloud projects add-iam-policy-binding --format=none $PROJECT_ID --member=serviceAccount:$SERVICE_ACCT_EMAIL --role='roles/aiplatform.user'\n",
        "!gcloud projects add-iam-policy-binding --format=none $PROJECT_ID --member=serviceAccount:$SERVICE_ACCT_EMAIL --role='roles/storage.objectViewer'\n",
        "\n",
        "time.sleep(60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfaINa3YxYQ3"
      },
      "source": [
        "### Create helper function to render images from Google Cloud Storage in this notebook\n",
        "\n",
        "This is a helpful utility function that you'll use later in the tutorial. It takes the results of your search query (stored in a pandas DataFrame) and displays the corresponding product images in a nice grid format, making it easy to see how well your search worked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g4dsEJKusGN"
      },
      "source": [
        "#### Import necessary libraries for the helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiepvKmLuyob"
      },
      "outputs": [],
      "source": [
        "# Imports for GCS, plotting, and image handling\n",
        "import io\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from google.cloud import storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfm005SWxhxg"
      },
      "source": [
        "#### Create a Google Cloud Storage client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUcaWbhWxkVv"
      },
      "outputs": [],
      "source": [
        "# Set variables\n",
        "REGION = \"US\"\n",
        "bucket = \"sample-data-and-media\"\n",
        "# Create Cloud Storage client\n",
        "client = storage.Client(project=PROJECT_ID)\n",
        "bucket = client.bucket(bucket)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6YFp-2Axer1"
      },
      "outputs": [],
      "source": [
        "def display_product_images(df):\n",
        "    \"\"\"Displays product images from a DataFrame in a 3-column grid.\n",
        "    Simplified for clarity in a tutorial context.\n",
        "    \"\"\"\n",
        "    # Calculate grid size\n",
        "    num_images = len(df)\n",
        "    cols = 3\n",
        "    rows = math.ceil(num_images / cols)\n",
        "\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(10, rows * 3.5))\n",
        "\n",
        "    # Flatten the axes array for easy iteration, regardless of shape\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        # Extract the image path from the full GCS URI\n",
        "        image_path = row[\"uri\"].split(\"/\", 3)[-1]\n",
        "\n",
        "        # Get the image from GCS\n",
        "        blob = bucket.blob(image_path)\n",
        "        image = Image.open(io.BytesIO(blob.download_as_bytes()))\n",
        "\n",
        "        # Plot the image\n",
        "        axs[i].imshow(image)\n",
        "        axs[i].set_title(image_path.split(\"/\")[-1], fontsize=10)\n",
        "        axs[i].axis(\"off\")\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for j in range(num_images, len(axs)):\n",
        "        axs[j].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HozZFfxrg9d_"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yla8q_2EFQl9"
      },
      "source": [
        "## Load and prepare the sample data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prepare_md"
      },
      "source": [
        "### Create a BigQuery dataset\n",
        "\n",
        "\n",
        "Before you load data into new tables, you need a place to store them. Running the following query will create a BigQuery dataset called **`cymbal_pets`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset_code"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "CREATE SCHEMA `cymbal_pets` OPTIONS (location = 'us');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFncMrqEGMAO"
      },
      "source": [
        "### Create the `products` table\n",
        "\n",
        "This query uses the `LOAD DATA` command to create a new table called `products` inside your `cymbal_pets` dataset. It simultaneously loads the table with product data from a Parquet file in Google Cloud Storage (GCS)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N94VjIMeX-F"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "LOAD DATA OVERWRITE cymbal_pets.products\n",
        "FROM FILES (\n",
        "  uris = ['gs://sample-data-and-media/cymbal-pets/tables/products'],\n",
        "  format = 'PARQUET'\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgSJkD3Q93l7"
      },
      "source": [
        "Run a quick `SELECT` query to view a couple of rows and confirm that your `products` table was created and loaded correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIKDAa-a0K13"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT *\n",
        "FROM cymbal_pets.products\n",
        "LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039VFiVBwzsF"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "## Perform text-to-text semantic search\n",
        "\n",
        "With the product data prepared, it's time to perform the first [semantic search](https://cloud.google.com/discover/what-is-semantic-search). You want to input a text query, like \"kitten toy\" and find the most similar products. To do this, you first need to convert the product names and descriptions into [vector embeddings](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings) -- numerical representations that capture their semantic meaning. Once the embeddings are generated, you can run a vector search.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8yup-saGjtc"
      },
      "source": [
        "### Create remote model for text embedding\n",
        "\n",
        "Begin by creating a remote model in BigQuery named `text_embedding_model`. This command doesn't create a new model; instead, it creates a pointer to an existing, pre-trained text embedding model hosted on Vertex AI ([`gemini-embedding-001`](https://ai.google.dev/gemini-api/docs/embeddings#model-versions)). This allows you to leverage Vertex AI models directly from your SQL queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHc6MHazGqR9"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "CREATE OR REPLACE MODEL `cymbal_pets.text_embedding_model`\n",
        "  REMOTE WITH CONNECTION `us.cymbal_conn`\n",
        "  OPTIONS (ENDPOINT = 'gemini-embedding-001');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgzCgXcUwcm4"
      },
      "source": [
        "### Generate text embeddings for product descriptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtgmxoax-pUT"
      },
      "source": [
        " To store embeddings, alter the `products` table to add a new column. Since embeddings are numerical representations of text (an array of floating point numbers), the column type will be `ARRAY<FLOAT>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "614L4qqk4ePX"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "ALTER TABLE cymbal_pets.products\n",
        "ADD COLUMN text_embedding ARRAY<FLOAT64>;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOo1l6rdOYk8"
      },
      "source": [
        "Now, generate embeddings with the [`ML.GENERATE_EMBEDDING`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-embedding) function. The following query passes the `product_name` and `description` to the `text_embedding_model` for each item. An `UPDATE` statement saves the resulting embeddings in a new column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLKxq9UrwreS"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "UPDATE `cymbal_pets.products` AS t\n",
        "SET t.text_embedding = s.ml_generate_embedding_result\n",
        "FROM (\n",
        "  SELECT\n",
        "    product_id,\n",
        "    ml_generate_embedding_result\n",
        "  FROM\n",
        "    ML.GENERATE_EMBEDDING(\n",
        "      MODEL `cymbal_pets.text_embedding_model`,\n",
        "      (\n",
        "        SELECT\n",
        "          product_id,\n",
        "          CONCAT(product_name, ' ', description) AS content\n",
        "        FROM `cymbal_pets.products`\n",
        "      ),\n",
        "      STRUCT(TRUE AS flatten_json_output)\n",
        "    )\n",
        ") AS s\n",
        "WHERE t.product_id = s.product_id;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FylUH24h_kXN"
      },
      "source": [
        "You should now see the `text_embedding` column populated with a long array of numbers. Those are vector embeddings that numerically represent each products' name and description!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdwWBdL-5yiP"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT product_id, product_name, description, text_embedding\n",
        "FROM cymbal_pets.products\n",
        "LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBBg2qw0_7Q-"
      },
      "source": [
        "### Use `VECTOR SEARCH` to perform a text-to-text search\n",
        "\n",
        "Now, use [`VECTOR_SEARCH`](https://cloud.google.com/bigquery/docs/reference/standard-sql/search_functions#vector_search) to perform a text-to-text search. This function works in two main steps:\n",
        "* A subquery generates an embedding for a search query (e.g. \"kitten toy\")\n",
        "* `VECTOR_SEARCH` then compares this new embedding against all existing product embeddings and returns the `top_k` (3 in this case) most similar results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bPZj0FM5heu"
      },
      "outputs": [],
      "source": [
        "%%bigquery text_matches --project {PROJECT_ID}\n",
        "\n",
        "SELECT base.product_id, base.product_name, base.brand, base.category, base.description, base.average_rating, base.uri, distance\n",
        "    FROM\n",
        "      VECTOR_SEARCH(\n",
        "        TABLE `cymbal_pets.products`,\n",
        "        'text_embedding',\n",
        "        (\n",
        "        -- GENERATE AN EMBEDDING AS A SUBQUERY\n",
        "        SELECT\n",
        "          ml_generate_embedding_result,\n",
        "          content AS query\n",
        "        FROM\n",
        "          ML.GENERATE_EMBEDDING(\n",
        "            MODEL `cymbal_pets.text_embedding_model`,\n",
        "            ( SELECT \"kitten toy\" AS content)\n",
        "          )\n",
        "        ),\n",
        "        top_k => 3)\n",
        "ORDER BY distance ASC;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzaZ3Gs2ASLu"
      },
      "source": [
        "The results of the vector search are now stored in a pandas DataFrame called `text_matches` (a parameter added to the [`%%bigquery` magic](https://cloud.google.com/python/docs/reference/bigquery/latest/magics) utility in the prior cell).\n",
        "\n",
        "Let's display the top rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1SlsPNz79nk"
      },
      "outputs": [],
      "source": [
        "text_matches.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Ev_nYsAjVI"
      },
      "source": [
        "### View search results\n",
        "Now you can use the helper function to display the images of the top 3 products the vector search returned. As you can see, the results are semantically related to \"kitten toy\" even if the exact words don't appear in the product names or descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlCaWeq78Aaz"
      },
      "outputs": [],
      "source": [
        "display_product_images(text_matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ytmT_Q6-J2I"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "## Perform text-to-image (multimodal) semantic search\n",
        "\n",
        "The previous example performed a text-to-text search. Now we can look at multimodal search. This allows you to use a text query to find products based on their visual characteristics (not using any written descriptions). Note that multimodal search also extends to audio, video, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y31x-x_HC92"
      },
      "source": [
        "### Create remote model for multimodal embedding\n",
        "\n",
        "To work with images, you'll need a different embedding model. You'll now create a remote model pointing to [`multimodalembedding@001`](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings#supported-models). Unlike the previous text-only model, this one is **multimodal**, meaning it can generate a single vector embedding from content that's text, an image, a video, audio, or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpJwFmAOGrXt"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "CREATE OR REPLACE MODEL `cymbal_pets.mm_embedding_model`\n",
        "  REMOTE WITH CONNECTION `us.cymbal_conn`\n",
        "  OPTIONS (ENDPOINT = 'multimodalembedding@001');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86UojNj1HLGB"
      },
      "source": [
        "### Generate multimodal embeddings for product images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C603D8_wBLpv"
      },
      "source": [
        "You need a new column in your table to store the embeddings generated from the product images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLliKpuc-Xra"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "ALTER TABLE cymbal_pets.products\n",
        "ADD COLUMN mm_embedding ARRAY<FLOAT64>;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqON6RSYBW-y"
      },
      "source": [
        "This step generates embeddings directly from product images. The process again uses [`ML.GENERATE_EMBEDDING`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-embedding), but with the `mm_embedding_model`.\n",
        "\n",
        "The key difference is how you provide the image to the model. Instead of a text string, the model requires a temporary, secure URL for each image. Because we haven't stored [`ObjectRef`](https://cloud.google.com/bigquery/docs/reference/standard-sql/objectref_functions)s in our table, we can generate them on the fly by chaining three functions together:\n",
        "\n",
        "* [`OBJ.MAKE_REF`](https://cloud.google.com/bigquery/docs/reference/standard-sql/objectref_functions#objmake_ref): First, it converts the GCS path from the `uri` column into a structured `ObjectRef`\n",
        "* [`OBJ.FETCH_METADATA`](https://cloud.google.com/bigquery/docs/reference/standard-sql/objectref_functions#objfetch_metadata): Next, it populates the `ObjectRef` with file metadata from Cloud Storage\n",
        "* [`OBJ.GET_ACCESS_URL`](https://cloud.google.com/bigquery/docs/reference/standard-sql/objectref_functions#objget_access_url): Finally, it generates a signed URL from the metadata-rich `ObjectRef` that the model can access.\n",
        "\n",
        "The resulting vector embedding for each image is then saved to the `mm_embedding` column with an `UPDATE` statement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLVJ0o9s-fEJ"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "UPDATE `cymbal_pets.products` AS t\n",
        "SET\n",
        "  t.mm_embedding = s.ml_generate_embedding_result\n",
        "FROM (\n",
        "  SELECT\n",
        "    product_id,\n",
        "    ml_generate_embedding_result\n",
        "  FROM\n",
        "    ML.GENERATE_EMBEDDING(\n",
        "      MODEL `cymbal_pets.mm_embedding_model`,\n",
        "      (\n",
        "        SELECT\n",
        "          product_id,\n",
        "          OBJ.GET_ACCESS_URL(OBJ.FETCH_METADATA(OBJ.MAKE_REF(uri, 'us.cymbal_conn')), 'r') AS content\n",
        "        FROM\n",
        "          `cymbal_pets.products`\n",
        "      ),\n",
        "      STRUCT(TRUE AS flatten_json_output)\n",
        "    )\n",
        ") AS s\n",
        "WHERE t.product_id = s.product_id;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhJ9OzDmCANS"
      },
      "source": [
        " Inspect the table one more time to see your new `mm_embedding` column filled with multimodal embeddings, derived from product images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j2I3Hk3ANF6"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT product_id, product_name, description, mm_embedding\n",
        "FROM cymbal_pets.products\n",
        "LIMIT 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoy6iKuXCKI4"
      },
      "source": [
        "###Use `VECTOR SEARCH` to perform a text-to-image search\n",
        "\n",
        "The power of a multimodal model is that it places both text and images into the same \"embedding space.\" This allows you to use an embedding generated from a text string (like \"kitten toy\") and directly compare it against embeddings generated from images.\n",
        "\n",
        "The process for this text-to-image search mirrors the previous text-to-text search, with two key differences:\n",
        "* A text query (\"kitten toy\") is embedded using the multimodal `mm_embedding_model`\n",
        "* The search is performed against the image embedding column (`embedding_mm_image`) to find the most visually similar items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiC0CLa2AAlT"
      },
      "outputs": [],
      "source": [
        "%%bigquery multimodal_matches --project {PROJECT_ID}\n",
        "\n",
        "SELECT base.product_id, base.product_name, base.brand, base.category, base.description, base.average_rating, base.uri, distance\n",
        "    FROM\n",
        "      VECTOR_SEARCH(\n",
        "        TABLE`cymbal_pets.products`,\n",
        "        'mm_embedding',\n",
        "        (\n",
        "        SELECT\n",
        "          ml_generate_embedding_result,\n",
        "          content AS query\n",
        "        FROM\n",
        "          ML.GENERATE_EMBEDDING(\n",
        "            MODEL `cymbal_pets.mm_embedding_model`,\n",
        "            (SELECT \"kitten toy\" AS content)\n",
        "          )\n",
        "        ),\n",
        "        'ml_generate_embedding_result',\n",
        "        top_k => 3)\n",
        "ORDER BY distance ASC;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma7A9jF0HbUg"
      },
      "source": [
        "### View search results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1SRowjwCsda"
      },
      "source": [
        "Finally, let's visualize the results of the text-to-image search. You should see images of products that are conceptually related to \"kitten toy,\" demonstrating the power of multimodal embeddings to find relevant items based on visual similarity to a text description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNOV5oYY3I74"
      },
      "outputs": [],
      "source": [
        "# Display the results\n",
        "display_product_images(multimodal_matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup_md"
      },
      "source": [
        "---\n",
        "\n",
        "## Cleaning Up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup_code"
      },
      "outputs": [],
      "source": [
        "# Delete the BigQuery table\n",
        "! bq rm --table -f cymbal_pets.products\n",
        "\n",
        "# Delete the remote models\n",
        "! bq rm --model -f cymbal_pets.text_embedding_model\n",
        "! bq rm --model -f cymbal_pets.mm_embedding_model\n",
        "\n",
        "# Delete the remote connection\n",
        "! bq rm --connection --project_id=$PROJECT_ID --location=us cymbal_conn\n",
        "\n",
        "# Delete the BigQuery dataset\n",
        "! bq rm -r -f $PROJECT_ID:cymbal_pets"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "bigquery_embeddings_vector_search.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}